{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f39c5844",
   "metadata": {},
   "source": [
    "Tokenization: Splitting text into meaningful segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56142037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab1cde8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fbd5c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "will\n",
      "be\n",
      "going\n",
      "to\n",
      "bangalore\n",
      "soon\n",
      "in\n",
      "april\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I will be going to bangalore soon in april\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03af4f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c9fc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f901f227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "will be going to"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substr = doc[1:5]\n",
    "substr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e21816e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(substr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "872ce84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e369f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Token as lot of method like is alpha, like num, is email etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44802604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4eafbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "प्रथम\n",
      "False True\n",
      "एनएलपी\n",
      "False False\n",
      "कक्षा\n",
      "False False\n",
      "में\n",
      "False False\n",
      "आपका\n",
      "False False\n",
      "स्वागत\n",
      "False False\n",
      "है\n",
      "False False\n",
      "।\n",
      "False False\n",
      "मुझे\n",
      "False False\n",
      "मेरे\n",
      "False False\n",
      "5000\n",
      "False True\n",
      "रुपये\n",
      "False False\n",
      "वापस\n",
      "False False\n",
      "चाहिए\n",
      "False False\n",
      "।\n",
      "False False\n"
     ]
    }
   ],
   "source": [
    "nlp_hindi = spacy.blank(\"hi\")\n",
    "\n",
    "doc = nlp_hindi(\"प्रथम एनएलपी कक्षा में आपका स्वागत है। मुझे मेरे 5000 रुपये वापस चाहिए।\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)\n",
    "    print(token.is_currency, token.like_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "440247f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gimme\n",
      "double\n",
      "chesse\n",
      "extra\n",
      "large\n",
      "healthy\n",
      "pizza\n"
     ]
    }
   ],
   "source": [
    "#Custom rules\n",
    "\n",
    "sample_string = \"gimme double chesse extra large healthy pizza\"\n",
    "\n",
    "for tok in nlp(sample_string):\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9d7664a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gim\n",
      "me\n",
      "double\n",
      "chesse\n",
      "extra\n",
      "large\n",
      "healthy\n",
      "pizza\n"
     ]
    }
   ],
   "source": [
    "#now we want to add a custom rule to make gimme into give me, since it is not a valid word\n",
    "# so we can add a custom rule\n",
    "# we can't provide 'give' in the rule as it is not a part of the word\n",
    "#but we can provide rule to split it as gim and me (split single word into simpler words)    \n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\", [\n",
    "    {ORTH: \"gim\"},\n",
    "    {ORTH: \"me\"}\n",
    "]\n",
    ")\n",
    "\n",
    "for tok in nlp(sample_string):\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8efc2",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a108df0",
   "metadata": {},
   "source": [
    "(1) Think stats is a free book to study statistics (https://greenteapress.com/thinkstats2/thinkstats2.pdf)\n",
    "\n",
    "This book has references to many websites from where you can download free datasets. You are an NLP engineer working for some company and you want to collect all dataset websites from this book. To keep exercise simple you are given a paragraph from this book and you want to grab all urls from this paragraph using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04a773a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[http://www.data.gov/, http://www.science.gov/, http://data.gov.uk/.Two, http://www3.norc.org/gss+website/, http://www.europeansocialsurvey.org/.]\n"
     ]
    }
   ],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "\n",
    "# TODO: Write code here\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "url_list = []\n",
    "for token in nlp(text.replace('\\n','')):\n",
    "    if token.like_url:\n",
    "        url_list.append(token)\n",
    "        \n",
    "print(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c37abf",
   "metadata": {},
   "source": [
    "(2) Extract all money transaction from below sentence along with currency. Output should be,\n",
    "\n",
    "two $\n",
    "\n",
    "500 €"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b66b2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[two $, 500 €]\n"
     ]
    }
   ],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "\n",
    "# TODO: Write code here\n",
    "\n",
    "doc = nlp(transactions)\n",
    "currency_list = []\n",
    "for idx, token in enumerate(doc):\n",
    "    if token.is_currency and doc[idx-1].like_num:\n",
    "        currency_list.append(doc[idx-1: idx+1])\n",
    "        \n",
    "print(currency_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b064e146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
